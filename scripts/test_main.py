#!/usr/bin/env python3
"""
æµ‹è¯•main.pyè„šæœ¬
ç¡®ä¿åœ¨cuda:2ä¸Šæ­£å¸¸è¿è¡Œ
"""

import sys
import os
import torch
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))

def test_cuda_availability():
    """æµ‹è¯•CUDAå¯ç”¨æ€§"""
    print("ğŸ” æµ‹è¯•CUDAå¯ç”¨æ€§")
    print("=" * 60)
    
    print(f"PyTorchç‰ˆæœ¬: {torch.__version__}")
    print(f"CUDAå¯ç”¨: {torch.cuda.is_available()}")
    
    if torch.cuda.is_available():
        print(f"CUDAè®¾å¤‡æ•°é‡: {torch.cuda.device_count()}")
        for i in range(torch.cuda.device_count()):
            print(f"è®¾å¤‡ {i}: {torch.cuda.get_device_name(i)}")
            print(f"  å†…å­˜: {torch.cuda.get_device_properties(i).total_memory / 1024**3:.1f} GB")
        
        # æµ‹è¯•cuda:2
        if torch.cuda.device_count() > 2:
            print(f"\nğŸ¯ æµ‹è¯•cuda:2...")
            device = torch.device('cuda:2')
            print(f"ä½¿ç”¨è®¾å¤‡: {device}")
            
            # åˆ›å»ºæµ‹è¯•å¼ é‡
            test_tensor = torch.randn(1000, 1000).to(device)
            print(f"æµ‹è¯•å¼ é‡è®¾å¤‡: {test_tensor.device}")
            print(f"æµ‹è¯•å¼ é‡å½¢çŠ¶: {test_tensor.shape}")
            
            # ç®€å•è®¡ç®—æµ‹è¯•
            result = torch.matmul(test_tensor, test_tensor.T)
            print(f"è®¡ç®—ç»“æœè®¾å¤‡: {result.device}")
            print(f"è®¡ç®—ç»“æœå½¢çŠ¶: {result.shape}")
            
            print("âœ… cuda:2 å·¥ä½œæ­£å¸¸")
            return True
        else:
            print("âŒ æ²¡æœ‰è¶³å¤Ÿçš„CUDAè®¾å¤‡ï¼Œcuda:2ä¸å¯ç”¨")
            return False
    else:
        print("âŒ CUDAä¸å¯ç”¨")
        return False


def test_main_imports():
    """æµ‹è¯•main.pyçš„å¯¼å…¥"""
    print("\n" + "=" * 60)
    print("ğŸ” æµ‹è¯•main.pyå¯¼å…¥")
    print("=" * 60)
    
    try:
        # æµ‹è¯•ä¸»è¦æ¨¡å—å¯¼å…¥
        from src.data.data_manager import DataManager
        from src.utils.logger import Logger
        from src.models.models import ModelFactory
        from src.trainer.train import Trainer
        from src.utils.timer import Timer
        from src.utils.set_seed import set_seed
        
        print("âœ… æ‰€æœ‰ä¸»è¦æ¨¡å—å¯¼å…¥æˆåŠŸ")
        
        # æµ‹è¯•é…ç½®åŠ è½½
        import yaml
        with open('src/config.yaml', 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
        
        print(f"âœ… é…ç½®æ–‡ä»¶åŠ è½½æˆåŠŸ")
        print(f"è®¾å¤‡é…ç½®: {config['Train']['device']}")
        
        return True, config
        
    except Exception as e:
        print(f"âŒ å¯¼å…¥å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False, None


def test_data_loading_with_cuda():
    """æµ‹è¯•æ•°æ®åŠ è½½å’ŒCUDA"""
    print("\n" + "=" * 60)
    print("ğŸ” æµ‹è¯•æ•°æ®åŠ è½½å’ŒCUDA")
    print("=" * 60)
    
    try:
        from src.data.data_manager import DataManager
        from src.utils.logger import Logger
        import yaml
        
        # åŠ è½½é…ç½®
        with open('src/config.yaml', 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
        
        # åˆ›å»ºDataManager
        logger = Logger(name='test_main', config=config)
        data_path = config['Data']['data_path']
        data_manager = DataManager(config, logger, data_path)
        
        print("âœ… DataManageråˆ›å»ºæˆåŠŸ")
        
        # æµ‹è¯•DataLoader
        device = torch.device(config['Train']['device'])
        print(f"ä½¿ç”¨è®¾å¤‡: {device}")
        
        # è·å–ä¸€ä¸ªbatchå¹¶ç§»åŠ¨åˆ°GPU
        for batch_idx, (features, targets) in enumerate(data_manager.dataloader_train):
            features_gpu = features.to(device)
            targets_gpu = targets.to(device)
            
            print(f"ğŸ“Š Batch {batch_idx}:")
            print(f"   ç‰¹å¾è®¾å¤‡: {features_gpu.device}")
            print(f"   ç›®æ ‡è®¾å¤‡: {targets_gpu.device}")
            print(f"   ç‰¹å¾å½¢çŠ¶: {features_gpu.shape}")
            print(f"   ç›®æ ‡å½¢çŠ¶: {targets_gpu.shape}")
            
            # ç®€å•è®¡ç®—æµ‹è¯•
            dummy_output = torch.randn_like(targets_gpu)
            loss = torch.nn.functional.mse_loss(dummy_output, targets_gpu)
            print(f"   æŸå¤±å€¼: {loss.item():.6f}")
            print(f"   æŸå¤±è®¾å¤‡: {loss.device}")
            
            if batch_idx >= 1:  # åªæµ‹è¯•å‰2ä¸ªbatch
                break
        
        print("âœ… æ•°æ®åŠ è½½å’ŒCUDAæµ‹è¯•æˆåŠŸ")
        return True
        
    except Exception as e:
        print(f"âŒ æ•°æ®åŠ è½½å’ŒCUDAæµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


def test_model_creation():
    """æµ‹è¯•æ¨¡å‹åˆ›å»º"""
    print("\n" + "=" * 60)
    print("ğŸ” æµ‹è¯•æ¨¡å‹åˆ›å»º")
    print("=" * 60)
    
    try:
        from src.models.models import ModelFactory
        from src.utils.logger import Logger
        import yaml
        
        # åŠ è½½é…ç½®
        with open('src/config.yaml', 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
        
        logger = Logger(name='test_model', config=config)
        
        # åˆ›å»ºæ¨¡å‹
        model_factory = ModelFactory(logger=logger, phase=1, model_name='grmlp')
        model = model_factory.model
        
        print(f"âœ… æ¨¡å‹åˆ›å»ºæˆåŠŸ: {type(model).__name__}")
        
        # ç§»åŠ¨åˆ°GPU
        device = torch.device(config['Train']['device'])
        model = model.to(device)
        print(f"âœ… æ¨¡å‹ç§»åŠ¨åˆ°è®¾å¤‡: {next(model.parameters()).device}")
        
        # æµ‹è¯•å‰å‘ä¼ æ’­
        dummy_input = torch.randn(32, 3).to(device)
        with torch.no_grad():
            output = model(dummy_input)
        
        print(f"âœ… å‰å‘ä¼ æ’­æˆåŠŸ")
        print(f"   è¾“å…¥å½¢çŠ¶: {dummy_input.shape}")
        print(f"   è¾“å‡ºå½¢çŠ¶: {output.shape}")
        print(f"   è¾“å‡ºè®¾å¤‡: {output.device}")
        
        return True
        
    except Exception as e:
        print(f"âŒ æ¨¡å‹åˆ›å»ºæµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


def test_main_execution():
    """æµ‹è¯•main.pyæ‰§è¡Œ"""
    print("\n" + "=" * 60)
    print("ğŸ” æµ‹è¯•main.pyæ‰§è¡Œ")
    print("=" * 60)
    
    try:
        # è®¾ç½®ç¯å¢ƒå˜é‡
        os.environ['CUDA_VISIBLE_DEVICES'] = '2'
        
        # å¯¼å…¥mainæ¨¡å—
        import main
        
        print("âœ… main.pyå¯¼å…¥æˆåŠŸ")
        
        # æµ‹è¯•ä¸»è¦å‡½æ•°
        from main import set_logger, create_model, create_dataloaders
        
        # åŠ è½½é…ç½®
        import yaml
        with open('src/config.yaml', 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
        
        # æµ‹è¯•æ—¥å¿—å™¨
        logger = set_logger(config)
        print("âœ… æ—¥å¿—å™¨åˆ›å»ºæˆåŠŸ")
        
        # æµ‹è¯•æ•°æ®åŠ è½½å™¨
        train_loader, test_loader, eval_loader = create_dataloaders(logger, config)
        print("âœ… æ•°æ®åŠ è½½å™¨åˆ›å»ºæˆåŠŸ")
        
        # æµ‹è¯•æ¨¡å‹åˆ›å»º
        model = create_model(logger, 'grmlp', 1)
        print("âœ… æ¨¡å‹åˆ›å»ºæˆåŠŸ")
        
        print("âœ… main.pyæ‰€æœ‰ç»„ä»¶æµ‹è¯•æˆåŠŸ")
        return True
        
    except Exception as e:
        print(f"âŒ main.pyæ‰§è¡Œæµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸš€ å¼€å§‹main.pyæµ‹è¯•")
    print("=" * 80)
    
    # æµ‹è¯•ç»“æœç»Ÿè®¡
    test_results = []
    
    # æµ‹è¯•1: CUDAå¯ç”¨æ€§
    result1 = test_cuda_availability()
    test_results.append(("CUDAå¯ç”¨æ€§", result1))
    
    if not result1:
        print("\nâŒ CUDAä¸å¯ç”¨ï¼Œç»ˆæ­¢æµ‹è¯•")
        return
    
    # æµ‹è¯•2: å¯¼å…¥æµ‹è¯•
    result2, config = test_main_imports()
    test_results.append(("å¯¼å…¥æµ‹è¯•", result2))
    
    if not result2:
        print("\nâŒ å¯¼å…¥å¤±è´¥ï¼Œç»ˆæ­¢æµ‹è¯•")
        return
    
    # æµ‹è¯•3: æ•°æ®åŠ è½½å’ŒCUDA
    result3 = test_data_loading_with_cuda()
    test_results.append(("æ•°æ®åŠ è½½å’ŒCUDA", result3))
    
    # æµ‹è¯•4: æ¨¡å‹åˆ›å»º
    result4 = test_model_creation()
    test_results.append(("æ¨¡å‹åˆ›å»º", result4))
    
    # æµ‹è¯•5: main.pyæ‰§è¡Œ
    result5 = test_main_execution()
    test_results.append(("main.pyæ‰§è¡Œ", result5))
    
    # è¾“å‡ºæµ‹è¯•æ€»ç»“
    print("\n" + "=" * 80)
    print("ğŸ“‹ æµ‹è¯•æ€»ç»“")
    print("=" * 80)
    
    passed = 0
    total = len(test_results)
    
    for test_name, result in test_results:
        status = "âœ… é€šè¿‡" if result else "âŒ å¤±è´¥"
        print(f"{test_name:20} : {status}")
        if result:
            passed += 1
    
    print(f"\nğŸ“Š æµ‹è¯•ç»“æœ: {passed}/{total} é€šè¿‡")
    
    if passed == total:
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼main.pyå¯ä»¥åœ¨cuda:2ä¸Šæ­£å¸¸è¿è¡Œã€‚")
    else:
        print("âš ï¸  éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç›¸å…³é…ç½®ã€‚")


if __name__ == "__main__":
    main()
